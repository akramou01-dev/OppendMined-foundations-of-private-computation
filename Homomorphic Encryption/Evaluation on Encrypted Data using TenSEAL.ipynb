{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53fab7b",
   "metadata": {},
   "source": [
    "We are going to do some ML on encrypted data. We will train ML model using Pytorch on the TOX21 dataset from kaggle on plain data and doing the test on encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f6adcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tenseal as ts\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47071be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = th.load(\"data/train_X.pt\")\n",
    "x_test = th.load(\"data/test_X.pt\")\n",
    "y_train = th.load(\"data/train_y.pt\")\n",
    "y_test = th.load(\"data/test_y.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11f33495",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TensorDataset(x_train,y_train)\n",
    "train_dataloader = DataLoader(training_dataset,batch_size = 64)\n",
    "testing_dataset = TensorDataset(x_test,y_test)\n",
    "test_dataloader= DataLoader(testing_dataset,batch_size =1) \n",
    "\n",
    "\n",
    "\n",
    "# Training dataset\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "# Test dataset\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc209e1",
   "metadata": {},
   "source": [
    "for the activation fonction we are choosing the square fonction for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "759a4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.ln1 = nn.Linear(1024, 128)\n",
    "        self.ln2 = nn.Linear(128, 12)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = out * out\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f8145ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model,loss_fn,x_batch,y_batch ,opt=None,metric=None): \n",
    "    preds = model(x_batch)\n",
    "    loss  = loss_fn(preds,y_batch)\n",
    "    \n",
    "    if opt is not None: \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    metric_result = None \n",
    "    if metric is not None: \n",
    "        metric_result = metric(preds,y_batch)\n",
    "    return loss.item(), len(x_batch), metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "47554eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,lables):\n",
    "    outputs = th.sigmoid(outputs)\n",
    "    outputs = (outputs >= 0.5).int()\n",
    "    return torch.sum(outputs==lables).item() / len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfad77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56300826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs , model,device , loss_fn , opt , train_dl , metric = None): \n",
    "    for epoch in range(epochs):\n",
    "        losses,metrics = [],[]\n",
    "        for x_batch  , y_batch in train_dl:\n",
    "            x_batch , y_batch = x_batch.to(device), y_batch.to(device) \n",
    "            #training the model \n",
    "            loss ,_ ,metric_result = loss_batch(model,loss_fn,x_batch,y_batch , opt,metric=accuracy)\n",
    "            losses.append(loss)\n",
    "            metrics.append(metric_result)\n",
    "        val_metric = th.mean(th.tensor(metrics))\n",
    "        if metric is None: \n",
    "            print(f\"Epoch : {epoch+1}/{epochs} , Loss : {th.mean(th.tensor(losses))}\")\n",
    "        else  :\n",
    "            print(f\"Epoch : {epoch+1}/{epochs} , Loss : {th.mean(th.tensor(losses))} , {metric.__name__} : {val_metric} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5e2c1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/30 , Loss : 0.6855852007865906\n",
      "Epoch : 2/30 , Loss : 0.6699648499488831\n",
      "Epoch : 3/30 , Loss : 0.6549543142318726\n",
      "Epoch : 4/30 , Loss : 0.6405144333839417\n",
      "Epoch : 5/30 , Loss : 0.6266041398048401\n",
      "Epoch : 6/30 , Loss : 0.6131782531738281\n",
      "Epoch : 7/30 , Loss : 0.6001833081245422\n",
      "Epoch : 8/30 , Loss : 0.5875508785247803\n",
      "Epoch : 9/30 , Loss : 0.5751843452453613\n",
      "Epoch : 10/30 , Loss : 0.5629319548606873\n",
      "Epoch : 11/30 , Loss : 0.5505279898643494\n",
      "Epoch : 12/30 , Loss : 0.537453293800354\n",
      "Epoch : 13/30 , Loss : 0.5225729942321777\n",
      "Epoch : 14/30 , Loss : 0.503153920173645\n",
      "Epoch : 15/30 , Loss : 0.4725356996059418\n",
      "Epoch : 16/30 , Loss : 0.4198954701423645\n",
      "Epoch : 17/30 , Loss : 0.35275372862815857\n",
      "Epoch : 18/30 , Loss : 0.30621689558029175\n",
      "Epoch : 19/30 , Loss : 0.28526604175567627\n",
      "Epoch : 20/30 , Loss : 0.2755386531352997\n",
      "Epoch : 21/30 , Loss : 0.2697027921676636\n",
      "Epoch : 22/30 , Loss : 0.2654082477092743\n",
      "Epoch : 23/30 , Loss : 0.26187074184417725\n",
      "Epoch : 24/30 , Loss : 0.25877439975738525\n",
      "Epoch : 25/30 , Loss : 0.25596946477890015\n",
      "Epoch : 26/30 , Loss : 0.2533766031265259\n",
      "Epoch : 27/30 , Loss : 0.25095033645629883\n",
      "Epoch : 28/30 , Loss : 0.24866251647472382\n",
      "Epoch : 29/30 , Loss : 0.24649444222450256\n",
      "Epoch : 30/30 , Loss : 0.24443282186985016\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "lr = 0.01\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.SGD(params = model.parameters(),lr = lr)\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "fit(30,model,device,loss_fn,opt,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6d165b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.93\n"
     ]
    }
   ],
   "source": [
    "def compute_labels(out):\n",
    "    out = th.sigmoid(out)\n",
    "    return (out >= 0.5).int()\n",
    "\n",
    "\n",
    "# compute accuracy using hamming loss\n",
    "def accuracy(output, target):\n",
    "    # convert to labels\n",
    "    out = compute_labels(output)\n",
    "    # flatten and compute hamming loss\n",
    "    flat_out = out.flatten()\n",
    "    flat_target = target.flatten()\n",
    "    incorrect = th.logical_xor(flat_out, flat_target).sum().item()\n",
    "    hamming_loss = incorrect / len(flat_out)\n",
    "    return 1 - hamming_loss\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy(model(x_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d96e25",
   "metadata": {},
   "source": [
    "Now we define a PyTorch-like model, but which uses TenSEAL operations. During initialization, we fetch and store weights from PyTorch layers. The forward method will then use the stored weights to perform linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "db3aa2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HEModel:\n",
    "    def __init__(self, ln1, ln2):\n",
    "        self.ln1_weight = ln1.weight.t().tolist()\n",
    "        self.ln1_bias = ln1.bias.tolist()\n",
    "        self.ln2_weight = ln2.weight.t().tolist()\n",
    "        self.ln2_bias = ln2.bias.tolist()\n",
    "        \n",
    "    def forward(self, encrypted_vec):\n",
    "         #as the linear layer is : y = A*x + B where A is the weight and B is the bias so we are going to do this with the encrypted vector\n",
    "        encrypted_vec = encrypted_vec.mm(self.ln1_weight) + self.ln1_bias #the .mm() method is used for multuply matrix\n",
    "        encrypted_vec *= encrypted_vec # here is the activation fonction x*x\n",
    "        encrypted_vec = encrypted_vec.mm(self.ln2_weight) + self.ln2_bias\n",
    "        return encrypted_vec\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b93d4",
   "metadata": {},
   "source": [
    "Now we are going to choose the parameters and for that we saw previousely some intuition : \n",
    "    \n",
    "    in our model we have 3 multiplication , 1 for the activation fonction and 2 for the matmul (matrix multiplication) \n",
    "         operation so we need 3 bit scale at the middle of our coeff_mod_bit_size\n",
    "    the choice of the bit_scale can impact the precision of the franctionnal part, we must try diffrent values (so diffrent\n",
    "        precisions) \n",
    "    The last coefficient modulus should be higher than the bits_scale and the difference between them (5 here) impacts the precision of the integer part, but since we are deeling weth small numbers so 5 is enough\n",
    "    After choosing those parameters, we need to find the appropriate polynomial modulus degree to use, to guarantee 128-bits security. We can start with small power of two (4096) and go higher, till TenSEAL doesn't throw an error. It is also important to choose the polynomial modulus degree that allows us to put all our elements into a ciphertext.\n",
    "    Here, we need to put 1024 values, and anything above 2048 should make it, but only 8192 (and above) meet the security requirement.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "76726e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_scale = 30\n",
    "coeff_mod_bit_sizes = [50, bit_scale,bit_scale,bit_scale,50]\n",
    "poly_modulus_degree = 8192\n",
    "# for 8192 we can create a cipher text contains a vector of 8192//2 values, since we have juste 1024 so it much enogh\n",
    "context = ts.context(ts.SCHEME_TYPE.CKKS,poly_modulus_degree, coeff_mod_bit_sizes=coeff_mod_bit_sizes)\n",
    "context.global_scale =  2 ** bit_scale\n",
    "\n",
    "# Generate galois keys required for matmul in ckks_vector\n",
    "context.generate_galois_keys()\n",
    "\n",
    "he_model = HEModel(model.ln1, model.ln2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0948ed",
   "metadata": {},
   "source": [
    "Now we are doing the evaluation and for that we are going to encrypt the vector before passing it to the model to predict.\n",
    "after that we decrypt the results and compare it to the plain results\n",
    "\n",
    "if this evaluation is for 2 parties so the encrypted vector must be sent to the remote evalutation and the results are sent back for decryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98005a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "matches= 0\n",
    "he_outs= []\n",
    "start = time()\n",
    "for data , target in test_dataloader: \n",
    "    #flattened the data before processing\n",
    "    flat_vec = data.flatten()\n",
    "    #encryption of the vector\n",
    "    enc_vec = ts.ckks_vector(context,flat_vec)\n",
    "    #encryption eval \n",
    "    enc_out = he_model(enc_vec)\n",
    "    #decryption \n",
    "    # if there is 2 parties for evaluation so we stop here and re-send the enc_outs to the data owner for decrypting it and seyying the results \n",
    "    he_out = th.tensor(enc_out.decrypt())\n",
    "    he_outs.append(he_out.tolist())\n",
    "    #evalutation on plain data\n",
    "    plain_out = model(data)\n",
    "    # counting the matched labels\n",
    "    he_labels = compute_labels(he_out)\n",
    "    plain_labels = compute_labels(plain_out)\n",
    "    matches += (he_labels == plain_labels).sum().item()\n",
    "    \n",
    "end = time()\n",
    "\n",
    "print(f'this operation took {end-start} seconds')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "75385d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_labels(th.tensor([1.131231,-3.131414,0.05,0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "25bcdfde",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (216) must match the size of tensor b (9408) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15060/191755490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy on test set (encryption evalutaiton  ) : {accuracy(th.tensor(he_outs),y_test)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"encrypted evaluation matched {(matches / 12 * len(test_dataloader))*100}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15060/1693290769.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(output, target)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mflat_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mflat_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mincorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_xor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mhamming_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincorrect\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhamming_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (216) must match the size of tensor b (9408) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on test set (encryption evalutaiton  ) : {accuracy(th.tensor(he_outs),y_test)}\")\n",
    "print(f\"encrypted evaluation matched {(matches / 12 * len(test_dataloader))*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "11f71944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(he_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ce4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
